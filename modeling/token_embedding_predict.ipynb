{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3lDDn-gINwS",
        "outputId": "5eac1262-ab9f-45ba-8c5c-a5fa10c8a00c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmEN8z2FH3Rd",
        "outputId": "18b74cf8-9ec9-449f-9c1f-83a98b5109c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기업 순서: ['카카오', '우아한 형제들', '쿠팡', '라인', '네이버']\n",
            "유사도 확률 벡터: [0.20594607 0.20584546 0.20079741 0.19603445 0.19137664]\n",
            "확률의 합: 1.0000000298023224\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from konlpy.tag import Okt\n",
        "import re\n",
        "\n",
        "# 데이터프레임 읽기\n",
        "csv_file_path = \"/content/drive/MyDrive/output/crawl_merged_data.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# null 값을 빈 문자열로 대체하기\n",
        "df['Contents'].fillna('', inplace=True)\n",
        "\n",
        "# KoNLPy의 Okt 형태소 분석기를 사용하여 명사만 추출하여 토큰화하는 함수\n",
        "def model_tokenize_nouns(text):\n",
        "    okt = Okt()\n",
        "    text = re.sub(r\"[^가-힣a-zA-Z0-9]\", \" \", text)\n",
        "    nouns = okt.nouns(text)\n",
        "    return nouns\n",
        "\n",
        "# 데이터프레임의 텍스트 전처리 및 토큰화\n",
        "df['Tokenized_Nouns'] = df['Contents'].apply(model_tokenize_nouns)\n",
        "\n",
        "# 그룹화된 기업별로 텍스트를 모아서 Word2Vec 모델 학습\n",
        "company_embeddings = {}\n",
        "for company, group in df.groupby('Company'):\n",
        "    merged_text = ' '.join(group['Tokenized_Nouns'].sum())\n",
        "    tokens = merged_text.split()\n",
        "    model = Word2Vec([tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
        "    company_embeddings[company] = model\n",
        "\n",
        "# 새로운 자기소개서가 저장된 CSV 파일 경로\n",
        "new_resume_csv_path = \"/content/drive/MyDrive/output/new_resume.csv\"\n",
        "\n",
        "# CSV 파일 읽기\n",
        "new_resume_df = pd.read_csv(new_resume_csv_path)\n",
        "\n",
        "# 새로운 자기소개서 텍스트 추출\n",
        "new_resume_text = new_resume_df['Contents'].iloc[0]\n",
        "\n",
        "# 새로운 자기소개서 토큰화 함수\n",
        "new_resume_tokens = model_tokenize_nouns(new_resume_text)\n",
        "new_resume_model = Word2Vec([new_resume_tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# 기업별 자소서와의 유사도 계산\n",
        "similarities = {}\n",
        "for company, model in company_embeddings.items():\n",
        "    similarity = cosine_similarity([new_resume_model.wv[word] for word in new_resume_tokens if word in new_resume_model.wv],\n",
        "                                   [model.wv[word] for word in new_resume_tokens if word in model.wv])\n",
        "    similarities[company] = similarity[0][0]\n",
        "\n",
        "# 유사도가 높은 순으로 정렬\n",
        "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "# 코사인 유사도를 확률로 변환하는 softmax 함수\n",
        "def softmax(similarities):\n",
        "    exp_sim = np.exp(similarities)\n",
        "    return exp_sim / exp_sim.sum()\n",
        "\n",
        "# 유사도를 계산한 후 softmax 적용하여 유사도 확률 벡터 생성\n",
        "similarities = np.array([similarity for _, similarity in sorted_similarities])\n",
        "similarity_probabilities = softmax(similarities)\n",
        "# 기업 순서 출력\n",
        "company_order = [company for company, _ in sorted_similarities]\n",
        "print(f\"기업 순서: {company_order}\")\n",
        "print(f\"유사도 확률 벡터: {similarity_probabilities}\")\n",
        "print(f\"확률의 합: {sum(similarity_probabilities)}\")"
      ]
    }
  ]
}